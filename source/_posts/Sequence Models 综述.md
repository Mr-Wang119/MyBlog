---
title: GRU LSTM 综述
date: 2020-01-22 11:42:24
tags:
- 自然语言处理
categories: 自然语言处理
---

> 总结于Deeplearning.ai

## 1. 符号表示

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200202213436.png)

<!-- more -->

| 符号              | 含义                                                  |
| ----------------- | ----------------------------------------------------- |
| $ x^{(t)} $       | 位置$ t $的单词的特征表示                             |
| $ y^{(t)} $       | 第$ i $个句子位置t应输出的数据，即目标值 (图中未标出) |
| $ T_x $           | 输入序列的长度                                        |
| $ T_y$            | 出序列的长度                                          |
| $ a^{(t)} $       | 来自第$ t $个时间步的信息                             |
| $ W_{ax} $        | 从$ x^{(t)} $到隐藏层的连接的一系列参数               |
| $ W_{aa},\ b_a $  | 激活值$ a^{(t)} $到下一个隐层的参数                   |
| $ W^{ya} $        | 输出值参数 (图中未画出)                               |
| $ \hat{y}^{(t)} $ | 位置t的输出数据                                       |

在表示单词时，可以使用one-hot表示法进行表示，one-hot列向量中只有一行为1，其余为0，这一行为这个列向量表示的单词。当遇见不属于词表的单词时，可以使用<UNK>的伪单词的列向量进行标记。

循环神经网络参数在每个时间步中共享，因此上图显示的是数据随时间的流向，实际输出数据重新进入原RNN cell，因此上图三个cell中的$ W_{aa}$和$ W_{ax} $是相同的。

## 2. 循环神经网络模型

### (1) 为什么使用循环网络模型？

* 每个输入与输出数据可以具有不同的长度，因此无法使用传统神经网络结构进行计算；
* 简单的神经网络结构无法共享从文本不同位置学到的特征。文本识别可以类比卷积，在句子中不同位置出现的单词应当能够自动识别。使用传统网络结构一方面无法捕捉这种序列内部的位置信息，另一方面由于使用one-hot编码，因此每个单词乘字典维度为输入的数量，参数过多不适合训练。

### (2) 神经网络结构

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200202213436.png)

以句子作为输入为例，循环网络模型首先输入第一个单词的特征表示$ x^{(1)} $，并计算得到结果$ y^{(1)} $，之后读取第二个单词的特征表示$ x^{(2)} $，只是为了获得序列位置信息，网络结构还需要输入来自第一个时间步的信息，具体而言即时间步1的激活值$ a^{(1)} $会传入时间步2，与$ x^{(2)} $共同计算得到第二个时间步的输出$ y^{(2)} $，以此类推。

在开始整个流程时，需要构造一个激活值$ a^{(0)} $，这个值将与第一个单词的特征表示$ x^{(1)} $共同参与计算第一个时间步的输出值$ y^{(1)} $，一般使用零向量，也可以随机使用其他方法进行初始化。

#### 正向传播

在正向传播过程的第$ t $个时间步中，先输入$ a^{(t-1)} $和$ x^{(t)} $，分别和各自的参数$ W_{aa} $和$ W_{ax} $相乘并相加，之后经过激活函数后得到激活值$ a^{(t)} $，成为下一个时间步的输入。同时计算得到这个时间步的输出值$ y^{(t)} $，即：
$$
a^{(t)}=g_1(W_{aa}a^{(t-1)}+W_{ax}x^{(t)}+b_a),
$$

$$
\hat{y}^{(t)}=g_2(W_{ya}a^{(t)}+b_y).
$$

循环神经网络的激活函数经常选用tanh，也可以视具体情况使用其他激活函数。选用哪种激活函数取决于实现什么功能，如果是二分类问题就可以使用sigmoid，k分类的话使用softmax是更好的选择。在上图中有两个激活函数，也就是式子中的$ g_1() $和$ g_2() $，上文所指的是$ g_2() $。

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200203110709.png)

上文中的公式可以进行简化，我们定义$ W_a $为$ W_{aa} $和$ W_{ax} $并列放置，即上图左侧两个矩形构成的矩阵，然后将$ a^{(t-1)} $和$ x^{(t)} $纵向放置，构成一个矩阵，记为$ [a^{(t-1)},x^{(t)}] $，因此可以将公式改写为：
$$
a^{(t)}=g_1(W_a[a^{(t-1)},x^{(t)}]+b_a),
$$

$$
\hat{y}^{(t)}=g_2(W_{ya}a^{(t)}+b_y).
$$

#### 反向传播

反向传播的代价函数可以使用softmax损失函数：

{% raw %}
$$
L^{(t)}(\hat{y}^{(t)},y^{(t)})=-\sum_i{y_i^{(t)}log\hat{y}_i^{(t)}}.
$$
{% endraw %}

这个损失函数对应的序列中的一个具体的词，然后我们定义整个序列的损失函数，将$ L $定义为：
$$
L(\hat{y},y)=\sum_{t=1}^{T_x}L^{(t)}(\hat{y}^{(t)},y^{(t)}).
$$
在这个计算中，首先计算各个时间步的损失函数结果，然后将他们加起来，得到最后的损失。这种计算损失的方式称为backpropagation through time。

## 3. 循环神经网络的梯度消失

基本的RNN算法存在很大的问题。举一个语言模型的例子，当看到句子"The cat, which already ate ......, was full."，句子的前后应当一致，即cat为单数，对应was。这个句子有长期的依赖，最前面的单词对后面的单词有影响。而传统的RNN算法不擅长捕获这种长期的影响。

具体的原因类似网络层数加深时的梯度消失，在经过多个时间步后并反向传播，输出$ \hat{y} $的梯度很难传播回去，因此很难影响前面层的计算。这说明，很难让一个神经网络意识到它需要记住单词是单数形式还是复数形式。无论后面单词的预测正确与否，这个错误都很难影响到前面的参数。

当然，也有可能遇到梯度爆炸的问题，模型没有收敛的趋势，而出现了很多NaN，这个时候可以使用梯度修剪，通过监测梯度向量，如果大于某个阈值，则缩放梯度向量，保证它不会太大。

## 4. GRU单元

Gated Recurrent Unit (GRU) 门控制单元改变了RNN的隐藏层，使其更适合捕捉深层的特征，改善了梯度消失问题。

当从左往右读一个句子时，GRU单元使用一个新的变量，称其为$c$，代表细胞（cell），即记忆细胞。记忆细胞可以提供记忆的能力，即在后面的时间步中记忆前面时间步的信息。在时间$ t $处，有记忆细胞$ c^{(t)}=a^{(t)} $。它们的值是一样的，不同的名称是为了区分激活值和记忆细胞。

在每一个时间步中，我们需要计算一个候选值$ \widetilde{c}^{(t)}=tanh(W_c[c^{(t-1)},x^{(t)}]+b_c) $，这个候选值和$ c^{(t)} $会被每个时间步选择使用哪一个。

GRU中最重要的是我们使用了一个门$ \Gamma_u $，其中$ \Gamma $代表门，$ u $表示更新门，这是一个0到1之间的值。这个值的计算为：
$$
\Gamma_u=\delta(W_u[c^{(t-1)},x^{(t)}]+b_u).
$$
因此，这个值是通过将计算结果带入sigmoid函数得到的。sigmoid使得大多数情况下，$ \Gamma_u $的输出基本上非常接近0或1。门的作用是控制是否需要更新记忆细胞的值。以上文的单数句子为例，GRU单元会记住这个句子使用了单数，从而在后面生成单词时选择其单数形式。可以通过下列的式子达到对句子中特定时间点的记忆：

{% raw %}
$$
c^{(t)}=\Gamma_u*\widetilde{c}^{(t)}+(1-\Gamma_u)*c^{(t-1)}.
$$
{% endraw %}

在这个式子中，如果激活值的维度为100维，那么$ c^{(t-1)} $，$ \widetilde{c}^{(t)} $和$ \Gamma_u $均为100维，因此上式中的$ * $为元素对应相乘，而不是矩阵乘法。

从这个式子中我们可以看到，当$\Gamma_u$的值接近1时，此时时间细胞会主要选择这个时间点计算的结果并存储在记忆细胞中，而当$ \Gamma_u $趋近于0时，$ c^{(t)} $会选择保留上一时间点的记忆$ c^{(t-1)} $，从而达到记忆某个时间点的目的。由于$ \Gamma_u $为一个向量，因此GRU单元可以选择保存这个向量中的部分值而更新其他值以达到保存部分特征并更新其他特征的目的。

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200203153623.png)

上图绘制了一个**简易的GRU单元**第t个时间步中计算过程为：

（1）输入上一个时间步的记忆细胞$ c^{(t-1)} $和这个时间步的输入$ x^{(t)} $，使用tanh函数计算得到候选值$ \widetilde{c}^{(t)} $；

（2）同样输入上一个时间步的记忆细胞$ c^{(t-1)} $和这个时间步的输入$ x^{(t)} $，使用sigmoid函数计算得到更新门值$ \Gamma_u $；

（3）进行记忆细胞值的更新。输入上一个时间步的记忆细胞$ c^{(t-1)} $和这个时间步的输入$ x^{(t)} $，以及上一步计算得到的更新门值$ \Gamma_u $，计算得到新的记忆细胞值$ c^{(t)} $；

（4）输入新的记忆细胞值$ c^{(t)} $，通过softmax函数生成这个时间步的输出$ \hat{y}^{(t)} $。

由于现在这些门很容易取到0值，因此更新的式子就会变成$ c^{(t)}=c^{(t-1)} $，这非常有利于维持细胞的值，因此就不会有梯度消失的问题了，因此允许神经网络运行在非常庞大的依赖词上。

对于**完整的GRU单元**，我们需要在计算记忆细胞的候选值时加入一个新的项$ \Gamma_r $，这个项是一个新的门，其中的$ r $可以理解为相关性(relevance)，这个门告诉了计算出的下一个$ c^{(t)} $的候选值$ \widetilde{c}^{(t)} $跟$ c^{(t-1)} $有多大的相关性。计算相关门的值需要一个新的参数矩阵$ W_r $，$ \Gamma_r=\sigma(W_r[c^{(t-1)},x^{(t)}]+b_r) $。因此完整的GRU单元的公式如下：

{% raw %}
$$
\widetilde{c}^{(t)}=tanh(W_c[\Gamma_r*c^{(t-1)},x^{(t)}]+b_c),
$$

$$
\Gamma_u=\sigma(W_u[c^{(t-1)},x^{(t)}]+b_u),
$$

$$
\Gamma_r=\sigma(W_r[c^{(t-1)},x^{(t)}]+b_r),
$$

$$
c^{(t)}=\Gamma_u*\widetilde{c}^{(t)}+(1-\Gamma_u)*c^{(t-1)}.
$$

{% endraw %}

## 5. LSTM单元

LSTM相比于GRU，更加强大和通用。

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200204142021.png)

上面为LSTM和GRU的主要式子对比。相比GRU，LSTM没有相关门$ \Gamma_r $，同时激活值$a^{(t)}$不等于记忆细胞值$ c^{(t)} $。像以前一样，我们有一个更新门$ \Gamma_u $和表示更新的参数$ W_u $，$ \Gamma_u=\sigma(W_u[a^{(t-1)},x^{(t)}]+b_u) $，即上图中1。LSTM的一个新特性是不止有更新门控制，在计算新的记忆细胞值时，LSTM使用两个门$ \Gamma_u $和$ \Gamma_f $来控制，而不是仅使用$ \Gamma_u $来控制（如图2，3）。其中$ \Gamma_f $为遗忘门，$ \Gamma_f=\sigma(W_f[a^{(t-1)},x^{(t)}]+b_f) $。还有一个新的输出门$ \Gamma_o=\sigma(W_o[a^{(t-1)},x^{(t)}]+b_o) $。

{% raw %}

于是，记忆细胞的更新值$ c^{(t)}=\Gamma_u*\widetilde{c}^{(t)}+\Gamma_f*c^{(t-1)} $。最后，$ a^{(t)}=c^{(t)} $的式子变成了$ a^{(t)}=\Gamma_o*c^{(t)} $。

{% endraw %}

![](https://leewangml.oss-cn-beijing.aliyuncs.com/images/20200204162338.png)

LSTM在时间步t的计算过程如下：

（1）输入激活值$ a^{(t-1)} $和输入数据$ x^{(t)} $，带入forget gate、update gate、output gate公式，计算得到值$ \Gamma_f $、$ \Gamma_u $和$ \Gamma_o $；

（2）输入激活值$ a^{(t-1)} $和输入数据$ x^{(t)} $，带入tanh公式，得到候选值$ \widetilde{c}^{(t)} $；

（3）利用 (1) 中计算的得到的$ \Gamma_f $、$ \Gamma_u $，（2）中计算得到的候选值$ \widetilde{c}^{(t)} $，以及上一个时间步记忆单元的值$ c^{(t-1)} $计算得到新的记忆单元值$ c^{(t)} $；

（4）输入输出门$ \Gamma_o $的值和 (3) 中计算得到的本时间步的记忆单元的值计算得到新的激活值$ a^{(t)} $；

（5）将新的激活值送入激活函数（例如softmax），得到输出值$ y^{(t)} $。

LSTM最常用的版本的门值不仅取决于$ a^{(t-1)} $和$x^{(t)}$，也可以偷窥一下$ c^{(t-1)} $的值，这叫做peephole connection。因此在计算三个门$ \Gamma_f $、$ \Gamma_u $和$ \Gamma_o $时，加入$ c^{(t-1)} $计算。实际上，$ c^{(t-1)} $中的每一个元素仅能硬系那个处于同一维度的门值，不能硬系那个所有元素，因此偷窥孔可以称为是一对一的。