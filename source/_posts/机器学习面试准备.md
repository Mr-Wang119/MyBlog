---
title: 机器学习面试准备
date: 2019-04-16 15:14:02
tags: 面试
categories: 图像
---

# 一、前言

为了面试，总结一下之前使用过的模型以及一些面试常问的问题。

# 二、相关模型

## 2.1 MobileNets

> MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks.
>
> <p align="right">----MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</p>

<!--more-->

MobileNets从字面上很好理解它想达到的效果，也就是做到"Mobile"，在一系列CNN变体中，模型一步步加深，直到ResNet，其网络层已达到152层，巨大的计算存储开销使得这些网络不能很好得应用于例如嵌入式等低功耗领域，因此引出MobileNets，其由Google于2017年提出，其优点在于使用 **深度可分离卷积(depthwise separable convolutions)** ，因此相比于使用一般卷机的网络结构，可以显著**降低参数个数**。

- 深度可分离卷积

深度可分离卷积在AlexNet已经有所应用，只是当时是为了解决在GPU上的训练问题，AlexNet无法放入单个GPU中进行训练，因此通过分离卷积，将conv2层设置group=2，把网络放在两个GPU上进行训练。深度可分离卷积**将标准卷积分解为深度卷积(depthwise convolution)和1x1卷积(在论文中称为pointwise convolution)**，并可证明这种分离方式可以降低计算量与模型大小。

设卷积的输入为F($D_F\times D_F\times M$)，stride=padding=1，输出为G($D_F\times D_F\times N$)。深度可分离卷积共分为两步：深度卷积为每个输入通道应用单个滤波器，并利用1x1卷积将深度卷积的输出进行线性结合。对于一个滤波器的深度卷积，卷积过程中第m个通道的output为：

$$ \hat{G}_{k,l,m}=\sum_{i,j}\hat{K}_{i,j,m}\cdot F_{k-1+i,l-1+j,m}, $$

其中，卷积核K($ D_K \times D_K \times M $)中的每个通道分别与输入通道分别相乘，可以看作少了标准卷积对每个通道求和。在这里，将每个通道联系起来的任务就交给了1x1卷积。1x1卷积将不同的通道求和，以收集每个点的特征。因此，通过深度卷积，我们得到的输出维度为$ D_K\times D_K \times N \times M $，其中$N$为滤波器的个数，然后通过1x1卷积，我们得到的输出维度为$ D_K\times D_K\times N $的输出，与标准卷积得到的结果维度相同。

因此，在MobileNet中，将普通卷积替换为了深度卷积，提升了运行效率，其中：

​	**普通卷积：3x3 Conv -> BN -> ReLU;**

​	**深度卷积：3x3 Depthwise Conv -> BN -> ReLU -> 1x1 Conv -> BN -> ReLU.**

（1）若为标准卷积，卷积核为K($D_K\times D_K \times M\times N$)，$M$为input channel个数，$N$为output channel个数，其计算成本为：

$$ D_K\cdot D_K\cdot M\cdot N\cdot D_F. $$

（2）若为深度可分离卷积，设一个卷积核为K($D_K\times D_K \times M \times N$)，$M$为input channel个数，N为output channel个数，其计算成本为：

Depthwise : $$ D_K\cdot D_K\cdot M\cdot D_F\cdot D_F; $$

Pointwise：$M\cdot N\cdot D_F \cdot D_F.$

则现在的计算量是更换之前的

$$ \frac{ D_K\cdot D_K\cdot M\cdot D_F \cdot D_F+M\cdot N\cdot D_F \cdot D_F}{ D_K\cdot D_K\cdot M\cdot N\cdot D_F }=\frac{1}{N}+\frac{1}{D_K^2}. $$

因此可以看到相比普通卷积，计算量有所下降。

在MobileNet除了有V1，还有V2版本，MobileNet V1有如下两个缺点：

（1）易发生特征退化的问题，经过实验发现，当输入特征维数较低时，当输出后使用ReLU，得到输出之后再利用广义逆矩阵映射回原维度时，数据坍塌。

（2）使用ReLU作为激活函数时，如果输出为0，则梯度为0，因此后续迭代无法改变此节点的值，当网络中存在一定量的梯度为0的结点后，则会对整个网络造成影响，这在维度较低的情况下尤为明显，而**通过ResNet的结构复用，可以很大程度上缓解这种特征退化问题**。通过跳过一些梯度为0的结点，使得网络整体参数可以继续更新。

因此MobileNet采用Inverted residual block结构，首先使用1x1卷积对输入维度进行提升，然后再接ReLU，减少对特征的破坏，并利用残差连接降低特征退化的影响。

​	**Inverted residual block：**

​	**C1:1x1 Conv(ReLU) -> 3x3 DWise Conv(ReLU) -> C2:1x1 Conv(Linear) -> add(C1,C2);**

​	**1x1 Conv(ReLU) -> 3x3 DWise Conv(ReLU), stride=2 -> conv 1x1 Conv(Linear).**

## 2.2 tinyYOLO

（未完待续...